{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Inference Pipeline with Scikit-learn and XGBoost\n",
    "\n",
    "Typically a Machine Learning (ML) process consists of few steps: data gathering with various ETL jobs, pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm. In many cases, when the trained model is used for processing real time or batch prediction requests, the model receives data in a format which needs to pre-processed (e.g. featurized) before it can be passed to the algorithm. In the following notebook, we will demonstrate how you can build your ML Pipeline leveraging the Sagemaker Scikit-learn container and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Data preprocessing and XGBoost) as an Inference Pipeline behind a single Endpoint for real time inference.\n",
    "\n",
    "We'll use Sagemaker's Scikit-learn container to featurize the dataset so that it can be used for training with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "from io import StringIO\n",
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import os \n",
    "np.random.seed(0)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%matplotlib inline\n",
    " \n",
    "region = boto3.Session().region_name    \n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = 'sagemaker-xgboost-sentiment-classification'                     \n",
    "prefix = 'sklearn-xgboost-sentiment-classification/inference-pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data and training the model \n",
    "\n",
    "### Downloading dataset \n",
    "\n",
    "Datasets were uploaded to an S3 bucket and downloaded into the notebook as well using the was command line. Only data in the notebook can be opened in pandas and viewed - this helped ensure that the training data were correct. The inference pipeline connects to the same S3 sources to train and not to the data in the notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/data/test.csv to ./test.csv\n",
      "download: s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/data/y_test.csv to ./y_test.csv\n",
      "download: s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/train/full_train.csv to ./full_train.csv\n"
     ]
    }
   ],
   "source": [
    "# do not run if data is already loaded into sagemaker notebook instance directory\n",
    "!aws s3 cp s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/data/test.csv .\n",
    "!aws s3 cp s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/data/y_test.csv .\n",
    "!aws s3 cp s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/train/full_train.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>aws_neg</th>\n",
       "      <th>aws_pos</th>\n",
       "      <th>aws_mix</th>\n",
       "      <th>target</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magnificent experience with great service and excellent food</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.977658</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breakfast good as usual, great looking over the bay.</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.983629</td>\n",
       "      <td>0.010093</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best hamburgers in town!</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.650549</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good service.  Good view.  Good food.</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.991041</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good service got my ribs within 10 minutes.  Hurry on don't miss out</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.978226</td>\n",
       "      <td>0.014726</td>\n",
       "      <td>1</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    text  \\\n",
       "0  Magnificent experience with great service and excellent food            \n",
       "1  Breakfast good as usual, great looking over the bay.                    \n",
       "2  Best hamburgers in town!                                                \n",
       "3  Good service.  Good view.  Good food.                                   \n",
       "4  Good service got my ribs within 10 minutes.  Hurry on don't miss out    \n",
       "\n",
       "   rating   aws_neg   aws_pos   aws_mix  target  weights  \n",
       "0  5       0.000408  0.977658  0.009516  1       0.082    \n",
       "1  4       0.000518  0.983629  0.010093  1       0.082    \n",
       "2  5       0.001825  0.650549  0.002832  1       0.082    \n",
       "3  4       0.000181  0.991041  0.005654  1       0.082    \n",
       "4  5       0.001218  0.978226  0.014726  1       0.082    "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to read and view data in sagemaker \n",
    "train = pd.read_csv('full_train.csv', encoding='utf-8', names=['text','rating', \n",
    "                       'aws_neg', 'aws_pos', 'aws_mix', 'target', 'weights'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/data/batch_train.csv to ./batch_train.csv\n"
     ]
    }
   ],
   "source": [
    "# connect the sagemaker session to the relevant files in S3\n",
    "!aws s3 cp s3://sagemaker-xgboost-sentiment-classification/sklearn-xgboost-sentiment-classification/inference-pipeline/data/batch_train.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>aws_neg</th>\n",
       "      <th>aws_pos</th>\n",
       "      <th>aws_mix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magnificent experience with great service and excellent food</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.977658</td>\n",
       "      <td>0.009516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breakfast good as usual, great looking over the bay.</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.983629</td>\n",
       "      <td>0.010093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best hamburgers in town!</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.650549</td>\n",
       "      <td>0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good service.  Good view.  Good food.</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.991041</td>\n",
       "      <td>0.005654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good service got my ribs within 10 minutes.  Hurry on don't miss out</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.978226</td>\n",
       "      <td>0.014726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    text  \\\n",
       "0  Magnificent experience with great service and excellent food            \n",
       "1  Breakfast good as usual, great looking over the bay.                    \n",
       "2  Best hamburgers in town!                                                \n",
       "3  Good service.  Good view.  Good food.                                   \n",
       "4  Good service got my ribs within 10 minutes.  Hurry on don't miss out    \n",
       "\n",
       "   rating   aws_neg   aws_pos   aws_mix  \n",
       "0  5       0.000408  0.977658  0.009516  \n",
       "1  4       0.000518  0.983629  0.010093  \n",
       "2  5       0.001825  0.650549  0.002832  \n",
       "3  4       0.000181  0.991041  0.005654  \n",
       "4  5       0.001218  0.978226  0.014726  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to read and view data in sagemaker \n",
    "batch_train = pd.read_csv('batch_train.csv', encoding='utf-8', names=['text','rating', \n",
    "                          'aws_neg', 'aws_pos', 'aws_mix'])\n",
    "batch_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to upload data from sagemaker instance to S3\n",
    "fObj = open(\"batch_train.csv\", 'rb')\n",
    "folder_name = 'train'\n",
    "key = os.path.join(prefix, folder_name, 'batch_train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Scikit-learn script to train with <a class=\"anchor\" id=\"create_sklearn_script\"></a>\n",
    "To run Scikit-learn on Sagemaker `SKLearn` Estimator with a script as an entry point. The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* SM_MODEL_DIR: A string representing the path to the directory to write model artifacts to. These artifacts are uploaded to S3 for model hosting.\n",
    "* SM_OUTPUT_DIR: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing two input channels, 'train' and 'test', were used in the call to the Chainer estimator's fit() method, the following will be set, following the format SM_CHANNEL_[channel_name]:\n",
    "\n",
    "* SM_CHANNEL_TRAIN: A string representing the path to the directory containing data in the 'train' channel\n",
    "* SM_CHANNEL_TEST: Same as above, but for the 'test' channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an argparse.ArgumentParser instance.\n",
    "\n",
    "An additional `custom.py` script was used to add customized transformation helper functions to the sklearn pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Scikit Estimator <a class=\"anchor\" id=\"create_sklearn_estimator\"></a>\n",
    "\n",
    "To run our Scikit-learn training script on SageMaker, we construct a `sagemaker.sklearn.estimator.sklearn` estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "#from custom import ItemSelector, TextFeatures, clean_text, expand_contractions\n",
    "\n",
    "script_path = 'sklearn-featurizer.py'\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    train_instance_type=\"ml.c5.xlarge\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    dependencies=['custom.py']) # add custom functions\n",
    "\n",
    "train_prefix = 'sklearn-xgboost-sentiment-classification/inference-pipeline/train'\n",
    "train_data = 's3://{}/{}/{}'.format(bucket, train_prefix, 'full_train.csv')\n",
    "\n",
    "# connect the sagemaker session to the relevant files in S3\n",
    "train_input = sagemaker.session.s3_input(train_data, distribution='FullyReplicated', \n",
    "                             content_type='text/csv', s3_data_type='S3Prefix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-08 19:12:54 Starting - Starting the training job...\n",
      "2019-10-08 19:13:14 Starting - Launching requested ML instances......\n",
      "2019-10-08 19:14:15 Starting - Preparing the instances for training......\n",
      "2019-10-08 19:15:15 Downloading - Downloading input data\n",
      "2019-10-08 19:15:15 Training - Downloading the training image.\u001b[31m2019-10-08 19:15:27,417 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:27,419 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:27,430 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:27,651 sagemaker-containers INFO     Module sklearn-featurizer does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:27,651 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:27,651 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:27,652 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: sklearn-featurizer\n",
      "  Building wheel for sklearn-featurizer (setup.py): started\n",
      "  Building wheel for sklearn-featurizer (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-featurizer: filename=sklearn_featurizer-1.0.0-py2.py3-none-any.whl size=7042 sha256=a84117c09cd2c5a2acb480a5fb15a68100fcd3e8f8fb4d69442352f85933511e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y8jdqp0n/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built sklearn-featurizer\u001b[0m\n",
      "\u001b[31mInstalling collected packages: sklearn-featurizer\u001b[0m\n",
      "\u001b[31mSuccessfully installed sklearn-featurizer-1.0.0\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:28,693 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-10-08 19:15:28,704 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"log_level\": 20,\n",
      "    \"module_name\": \"sklearn-featurizer\",\n",
      "    \"is_master\": true,\n",
      "    \"user_entry_point\": \"sklearn-featurizer.py\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hyperparameters\": {},\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2019-10-08-19-12-53-337\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-944828514909/sagemaker-scikit-learn-2019-10-08-19-12-53-337/source/sourcedir.tar.gz\",\n",
      "    \"master_hostname\": \"algo-1\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=sklearn-featurizer\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2019-10-08-19-12-53-337\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-944828514909/sagemaker-scikit-learn-2019-10-08-19-12-53-337/source/sourcedir.tar.gz\",\"module_name\":\"sklearn-featurizer\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sklearn-featurizer.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=sklearn-featurizer.py\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_HPS={}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-1-944828514909/sagemaker-scikit-learn-2019-10-08-19-12-53-337/source/sourcedir.tar.gz\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python3 -m sklearn-featurizer\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31m                                                text  rating  ...  target  weights\u001b[0m\n",
      "\u001b[31m0  Magnificent experience with great service and ...       5  ...       1    0.082\u001b[0m\n",
      "\u001b[31m1  Breakfast good as usual, great looking over th...       4  ...       1    0.082\u001b[0m\n",
      "\u001b[31m2                          Best hamburgers in town!        5  ...       1    0.082\u001b[0m\n",
      "\u001b[31m3             Good service.  Good view.  Good food.        4  ...       1    0.082\u001b[0m\n",
      "\u001b[31m4  Good service got my ribs within 10 minutes.  H...       5  ...       1    0.082\n",
      "\u001b[0m\n",
      "\u001b[31m[5 rows x 7 columns]\u001b[0m\n",
      "\n",
      "2019-10-08 19:15:27 Training - Training image download completed. Training in progress.\u001b[31msaved model!\u001b[0m\n",
      "\u001b[31m2019-10-08 19:16:27,446 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-10-08 19:16:48 Uploading - Uploading generated training model\n",
      "2019-10-08 19:16:48 Completed - Training job completed\n",
      "Training seconds: 104\n",
      "Billable seconds: 104\n"
     ]
    }
   ],
   "source": [
    "sklearn_preprocessor.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch transform our training data \n",
    "Now that our proprocessor is properly fitted, let's go ahead and preprocess our training data. Let's use batch transform to directly preprocess the raw data and store right back into s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn_preprocessor.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.c5.xlarge',\n",
    "    assemble_with = 'Line',\n",
    "    max_payload = 35,\n",
    "    accept = 'text/csv')\n",
    "\n",
    "# Increased waiting time and instance type because gateway \n",
    "# kept timing out\n",
    "transformer.env = {\"SAGEMAKER_MODEL_SERVER_TIMEOUT\" : \"3600\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for transform job: sagemaker-scikit-learn-2019-10-08-19-17-07-224\n",
      "................\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: sklearn-featurizer\n",
      "  Building wheel for sklearn-featurizer (setup.py): started\n",
      "  Building wheel for sklearn-featurizer (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn-featurizer: filename=sklearn_featurizer-1.0.0-py2.py3-none-any.whl size=7042 sha256=78894a6d10a24db32f1f0a65fae299fa3b5e6bf914ddf15ebe755baed1b2cfd5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0l8b55ct/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built sklearn-featurizer\u001b[0m\n",
      "\u001b[31mInstalling collected packages: sklearn-featurizer\u001b[0m\n",
      "\u001b[31mSuccessfully installed sklearn-featurizer-1.0.0\u001b[0m\n",
      "\u001b[31m[2019-10-08 19:19:30 +0000] [37] [INFO] Starting gunicorn 19.9.0\u001b[0m\n",
      "\u001b[31m[2019-10-08 19:19:30 +0000] [37] [INFO] Listening at: unix:/tmp/gunicorn.sock (37)\u001b[0m\n",
      "\u001b[31m[2019-10-08 19:19:30 +0000] [37] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[31m[2019-10-08 19:19:30 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[31m[2019-10-08 19:19:30 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[31m[2019-10-08 19:19:30 +0000] [46] [INFO] Booting worker with pid: 46\u001b[0m\n",
      "\u001b[31m[2019-10-08 19:19:30 +0000] [45] [INFO] Booting worker with pid: 45\u001b[0m\n",
      "\u001b[31m2019-10-08 19:19:51,097 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m169.254.255.130 - - [08/Oct/2019:19:20:02 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[31m169.254.255.130 - - [08/Oct/2019:19:20:02 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[31m2019-10-08 19:20:02,161 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m169.254.255.130 - - [08/Oct/2019:19:20:48 +0000] \"POST /invocations HTTP/1.1\" 200 171171202 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess training input\n",
    "transformer.transform(train_data, content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-944828514909/sagemaker-scikit-learn-2019-10-08-19-17-07-224'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGBoost Model with Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sagemaker_session.boto_region_name, 'xgboost', repo_version=\"0.90-1\")\n",
    "print (training_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-08 19:21:20 Starting - Starting the training job...\n",
      "2019-10-08 19:21:21 Starting - Launching requested ML instances......\n",
      "2019-10-08 19:22:23 Starting - Preparing the instances for training......\n",
      "2019-10-08 19:23:43 Downloading - Downloading input data\n",
      "2019-10-08 19:23:43 Training - Downloading the training image...\n",
      "2019-10-08 19:24:05 Training - Training image download completed. Training in progress.\u001b[31mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[31mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softprob to Json.\u001b[0m\n",
      "\u001b[31mReturning the value itself\u001b[0m\n",
      "\u001b[31mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[31mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[31m[19:24:07] 146567x63 matrix with 9233721 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,&weight_column=1\u001b[0m\n",
      "\u001b[31mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[31mINFO:root:Train matrix has 146567 rows\u001b[0m\n",
      "\u001b[31m[0]#011train-merror:0.127236\u001b[0m\n",
      "\u001b[31m[1]#011train-merror:0.126024\u001b[0m\n",
      "\u001b[31m[2]#011train-merror:0.122281\u001b[0m\n",
      "\u001b[31m[3]#011train-merror:0.119815\u001b[0m\n",
      "\u001b[31m[4]#011train-merror:0.118533\u001b[0m\n",
      "\u001b[31m[5]#011train-merror:0.118688\u001b[0m\n",
      "\u001b[31m[6]#011train-merror:0.116331\u001b[0m\n",
      "\u001b[31m[7]#011train-merror:0.115757\u001b[0m\n",
      "\u001b[31m[8]#011train-merror:0.114556\u001b[0m\n",
      "\u001b[31m[9]#011train-merror:0.114534\u001b[0m\n",
      "\u001b[31m[10]#011train-merror:0.114167\u001b[0m\n",
      "\u001b[31m[11]#011train-merror:0.113765\u001b[0m\n",
      "\u001b[31m[12]#011train-merror:0.113802\u001b[0m\n",
      "\u001b[31m[13]#011train-merror:0.113206\u001b[0m\n",
      "\u001b[31m[14]#011train-merror:0.113052\u001b[0m\n",
      "\u001b[31m[15]#011train-merror:0.112627\u001b[0m\n",
      "\u001b[31m[16]#011train-merror:0.112578\u001b[0m\n",
      "\u001b[31m[17]#011train-merror:0.112083\u001b[0m\n",
      "\u001b[31m[18]#011train-merror:0.111603\u001b[0m\n",
      "\u001b[31m[19]#011train-merror:0.111087\u001b[0m\n",
      "\u001b[31m[20]#011train-merror:0.110175\u001b[0m\n",
      "\u001b[31m[21]#011train-merror:0.109718\u001b[0m\n",
      "\u001b[31m[22]#011train-merror:0.109598\u001b[0m\n",
      "\u001b[31m[23]#011train-merror:0.109061\u001b[0m\n",
      "\u001b[31m[24]#011train-merror:0.108846\u001b[0m\n",
      "\u001b[31m[25]#011train-merror:0.108641\u001b[0m\n",
      "\u001b[31m[26]#011train-merror:0.107948\u001b[0m\n",
      "\u001b[31m[27]#011train-merror:0.1079\u001b[0m\n",
      "\u001b[31m[28]#011train-merror:0.107352\u001b[0m\n",
      "\u001b[31m[29]#011train-merror:0.107224\u001b[0m\n",
      "\u001b[31m[30]#011train-merror:0.106957\u001b[0m\n",
      "\u001b[31m[31]#011train-merror:0.106366\u001b[0m\n",
      "\u001b[31m[32]#011train-merror:0.105928\u001b[0m\n",
      "\u001b[31m[33]#011train-merror:0.105616\u001b[0m\n",
      "\u001b[31m[34]#011train-merror:0.105708\u001b[0m\n",
      "\u001b[31m[35]#011train-merror:0.105171\u001b[0m\n",
      "\u001b[31m[36]#011train-merror:0.104906\u001b[0m\n",
      "\u001b[31m[37]#011train-merror:0.104496\u001b[0m\n",
      "\u001b[31m[38]#011train-merror:0.10413\u001b[0m\n",
      "\u001b[31m[39]#011train-merror:0.103922\u001b[0m\n",
      "\u001b[31m[40]#011train-merror:0.103767\u001b[0m\n",
      "\u001b[31m[41]#011train-merror:0.103428\u001b[0m\n",
      "\u001b[31m[42]#011train-merror:0.103163\u001b[0m\n",
      "\u001b[31m[43]#011train-merror:0.103046\u001b[0m\n",
      "\u001b[31m[44]#011train-merror:0.102897\u001b[0m\n",
      "\u001b[31m[45]#011train-merror:0.102553\u001b[0m\n",
      "\u001b[31m[46]#011train-merror:0.101955\u001b[0m\n",
      "\u001b[31m[47]#011train-merror:0.101657\u001b[0m\n",
      "\u001b[31m[48]#011train-merror:0.101735\u001b[0m\n",
      "\u001b[31m[49]#011train-merror:0.101522\u001b[0m\n",
      "\u001b[31m[50]#011train-merror:0.101023\u001b[0m\n",
      "\u001b[31m[51]#011train-merror:0.100818\u001b[0m\n",
      "\u001b[31m[52]#011train-merror:0.100641\u001b[0m\n",
      "\u001b[31m[53]#011train-merror:0.100405\u001b[0m\n",
      "\u001b[31m[54]#011train-merror:0.100228\u001b[0m\n",
      "\u001b[31m[55]#011train-merror:0.100281\u001b[0m\n",
      "\u001b[31m[56]#011train-merror:0.099841\u001b[0m\n",
      "\u001b[31m[57]#011train-merror:0.099813\u001b[0m\n",
      "\u001b[31m[58]#011train-merror:0.09936\u001b[0m\n",
      "\u001b[31m[59]#011train-merror:0.099311\u001b[0m\n",
      "\u001b[31m[60]#011train-merror:0.099313\u001b[0m\n",
      "\u001b[31m[61]#011train-merror:0.098869\u001b[0m\n",
      "\u001b[31m[62]#011train-merror:0.098706\u001b[0m\n",
      "\u001b[31m[63]#011train-merror:0.098408\u001b[0m\n",
      "\u001b[31m[64]#011train-merror:0.098217\u001b[0m\n",
      "\u001b[31m[65]#011train-merror:0.098168\u001b[0m\n",
      "\u001b[31m[66]#011train-merror:0.097724\u001b[0m\n",
      "\u001b[31m[67]#011train-merror:0.097588\u001b[0m\n",
      "\u001b[31m[68]#011train-merror:0.097337\u001b[0m\n",
      "\u001b[31m[69]#011train-merror:0.09697\u001b[0m\n",
      "\u001b[31m[70]#011train-merror:0.096826\u001b[0m\n",
      "\u001b[31m[71]#011train-merror:0.096749\u001b[0m\n",
      "\u001b[31m[72]#011train-merror:0.096473\u001b[0m\n",
      "\u001b[31m[73]#011train-merror:0.096088\u001b[0m\n",
      "\u001b[31m[74]#011train-merror:0.095958\u001b[0m\n",
      "\u001b[31m[75]#011train-merror:0.095952\u001b[0m\n",
      "\u001b[31m[76]#011train-merror:0.095559\u001b[0m\n",
      "\u001b[31m[77]#011train-merror:0.095323\u001b[0m\n",
      "\u001b[31m[78]#011train-merror:0.095218\u001b[0m\n",
      "\u001b[31m[79]#011train-merror:0.094877\u001b[0m\n",
      "\u001b[31m[80]#011train-merror:0.094538\u001b[0m\n",
      "\u001b[31m[81]#011train-merror:0.09456\u001b[0m\n",
      "\u001b[31m[82]#011train-merror:0.094443\u001b[0m\n",
      "\u001b[31m[83]#011train-merror:0.094385\u001b[0m\n",
      "\u001b[31m[84]#011train-merror:0.094355\u001b[0m\n",
      "\u001b[31m[85]#011train-merror:0.094197\u001b[0m\n",
      "\u001b[31m[86]#011train-merror:0.093978\u001b[0m\n",
      "\u001b[31m[87]#011train-merror:0.0941\u001b[0m\n",
      "\u001b[31m[88]#011train-merror:0.093881\u001b[0m\n",
      "\u001b[31m[89]#011train-merror:0.093678\u001b[0m\n",
      "\u001b[31m[90]#011train-merror:0.093672\u001b[0m\n",
      "\u001b[31m[91]#011train-merror:0.093454\u001b[0m\n",
      "\u001b[31m[92]#011train-merror:0.093425\u001b[0m\n",
      "\u001b[31m[93]#011train-merror:0.093269\u001b[0m\n",
      "\u001b[31m[94]#011train-merror:0.092928\u001b[0m\n",
      "\u001b[31m[95]#011train-merror:0.092988\u001b[0m\n",
      "\u001b[31m[96]#011train-merror:0.092627\u001b[0m\n",
      "\u001b[31m[97]#011train-merror:0.092518\u001b[0m\n",
      "\u001b[31m[98]#011train-merror:0.092456\u001b[0m\n",
      "\u001b[31m[99]#011train-merror:0.092488\u001b[0m\n",
      "\n",
      "2019-10-08 19:25:35 Uploading - Uploading generated training model\n",
      "2019-10-08 19:25:35 Completed - Training job completed\n",
      "Training seconds: 129\n",
      "Billable seconds: 129\n"
     ]
    }
   ],
   "source": [
    "xgb_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.m4.xlarge',\n",
    "                                         train_volume_size = 2,\n",
    "                                         train_max_run = 3600,\n",
    "                                         input_mode= 'File',\n",
    "                                         sagemaker_session=sagemaker_session)\n",
    "\n",
    "xgb_model.set_hyperparameters(objective = \"multi:softprob\",\n",
    "                              base_score = 0,\n",
    "                              eta = .1,\n",
    "                              gamma = 0,\n",
    "                              max_depth = 3,\n",
    "                              num_round = 100,\n",
    "                              num_class=3,\n",
    "                              csv_weights=1,\n",
    "                              subsample = 1,\n",
    "                              silent = 0)\n",
    "\n",
    "preprocessed_train_data = sagemaker.session.s3_input(\n",
    "    preprocessed_train, \n",
    "    distribution='FullyReplicated',\n",
    "    content_type='text/csv', \n",
    "    s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': preprocessed_train_data}\n",
    "xgb_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.inputs.s3_input at 0x7f2098767ef0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Inference Pipeline with Scikit preprocessor and XGBoost Model\n",
    "\n",
    "### Set up the inference pipeline <a class=\"anchor\" id=\"pipeline_setup\"></a>\n",
    "Setting up a Machine Learning pipeline can be done with the Pipeline Model. This sets up a list of models in a single endpoint; in this example, we configure our pipeline model with the fitted Scikit-learn preprocessing step and the fitted xgboost model. Deploying the model follows the same ```deploy``` pattern in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "sklearn_preprocessing_model = sklearn_preprocessor.create_model()\n",
    "xgb_sentiment_model = xgb_model.create_model()\n",
    "\n",
    "# set environment variable in first container to ensure \n",
    "# the content coming out from the preprocessing model is in the right format\n",
    "sklearn_preprocessing_model.env = {\"SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT\":\"text/csv\"}\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "endpoint_name = 'inference-pipeline-ep-2019-10-07-23-33-51' #'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_pipeline = PipelineModel(\n",
    "    name=model_name, \n",
    "    role=role, \n",
    "    models=[\n",
    "        sklearn_preprocessing_model, \n",
    "        xgb_sentiment_model])\n",
    "\n",
    "sm_pipeline.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The endpoint was updated after being created because \n",
    "# the entry script was changed. \n",
    "endpoint_name = 'inference-pipeline-ep-2019-10-07-23-33-51'\n",
    "\n",
    "sm_pipeline.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \n",
    "                   endpoint_name=endpoint_name, update_endpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a request to our pipeline endpoint <a class=\"anchor\" id=\"pipeline_inference_request\"></a>\n",
    "\n",
    "Here we just grab the first line from the test data (you'll notice that the inference python script is very particular about the ordering of the inference request data). The ```ContentType``` field configures the first container, while the ```Accept``` field configures the last container. You can also specify each container's ```Accept``` and ```ContentType``` values using environment variables.\n",
    "\n",
    "We make our request with the payload in ```'text/csv'``` format, since that is what our script currently supports. If other formats need to be supported, this would have to be added to the ```output_fn()``` method in our entry point. Note that we set the ```Accept``` to ```text/csv```, since XGBoost does not support ```application/json``` ```Accept```. The prediction output in this case is trying to guess the number of rings the abalone specimen would have given its other physical features; the actual number of rings is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "\n",
    "payload = '\"The food was nice, element of improvement on the preparation of the ribs should be grilled. \\\n",
    "Please invest in a flat top griller. Use a different bbq sauce for the chicken wings and  ribs. \\\n",
    "Overall service from Andries was great. \", 4, 0.010453076, 0.754360855, 0.209270433'\n",
    "\n",
    "#payload = '\"Very good food and plenty parking .\" ,4,0.002821277,0.9654154779999999,0.00835315'\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name,\n",
    "                              sagemaker_session=sagemaker_session,\n",
    "                              serializer=csv_serializer,\n",
    "                              content_type=CONTENT_TYPE_CSV,\n",
    "                              accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint\n",
    "\n",
    "Only delete if no longer in use. To reactivate - rerun `sm_pipeline.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '80a5d025-a321-4d22-b80c-6215f0be5059',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '80a5d025-a321-4d22-b80c-6215f0be5059',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 08 Oct 2019 19:43:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smclient.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
